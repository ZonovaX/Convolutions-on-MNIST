{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb0372a-6544-41b7-8ced-99665ec6e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604b7449-1239-4565-be64-041b4bc8ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b8685-2e6f-4030-aa58-0ebbe74dd377",
   "metadata": {},
   "source": [
    "the 3 total conv layers are 1x16 -> 16x16 -> 16x10, and in the two conv case we use first and last <br>\n",
    "\n",
    "using lr = 0.001 <br>\n",
    "mnist6 (and mnist13) is with 2 conv layers with stride 1 and then AvgPool(4) into FC, ~6.5k param <br>\n",
    "mnist7 is with 2 conv layers with stride 2 and no pooling into FC ~6.5k param<br>\n",
    "mnist8 is with 3 conv layers with stride 2 and no pooling into FC ~5.5k param<br>\n",
    "mnist14 is with 2 conv layers with stride 1 and then MaxPool(4) into FC <br>\n",
    "using lr = 0.05 <br>\n",
    "mnist5 (and mnist10) is with all 3 conv layers at stride 2 and then 4 pooling, no FC, ~4k param <br>\n",
    "mnist9 is with 3 conv layers with stride 2 and no pooling into FC ~ 5.5k param <br>\n",
    "mnist11 is with 2 conv layers with stride 2 and no pooling into FC ~ 6.5k param <br>\n",
    "mnist12 is with 2 conv layers with stride 1 and then AvgPool(4) into an FC ~ 6.5k param <br>\n",
    "mnist15 is with 2 conv layers with stride 1 and then MaxPool(4) into an FC ~ 6.5k param <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc51d05-f765-4855-ba4d-1c9b3991beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "writer = SummaryWriter(\"runs/mnist15\")\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def init_weights(mod):\n",
    "    if (type(mod) == nn.Conv2d) or (type(mod) == nn.Linear):\n",
    "        torch.nn.init.orthogonal_(mod.weight)\n",
    "\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv3.weight)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return F.log_softmax(xb.view(-1, xb.size(1)), dim=1)\n",
    "\"\"\"\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.05\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\"\"\"\n",
    "\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        Lambda(preprocess),\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "        #nn.ReLU(),\n",
    "        nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.AvgPool2d(4),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(10*7*7,10),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "        Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    )\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    n_total_steps = len(train_dl)\n",
    "    running_loss = 0.0\n",
    "    list_loss=[]\n",
    "    list_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            i+=1\n",
    "            running_loss += loss_batch(model,loss_func,xb, yb, opt)[0]\n",
    "            if i%100==0:\n",
    "                writer.add_scalar('training loss',running_loss/100, epoch*n_total_steps + i)\n",
    "                list_loss.append(running_loss/100)\n",
    "                running_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        writer.add_scalar('testing acc', val_acc, epoch)\n",
    "        list_acc.append(val_acc)\n",
    "        print(epoch, val_loss,val_acc)\n",
    "    return list_loss,list_acc\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e35af2d-fb96-493e-978c-3039dd44fd94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dl, valid_dl \u001b[38;5;241m=\u001b[39m get_data(training_data,test_data,batch_size)\n\u001b[1;32m----> 2\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m example_data, example_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(examples)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Mnist_CNN()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "examples = iter(train_dl)\n",
    "example_data, example_targets = next(examples)\n",
    "model = Mnist_CNN()\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images_5',img_grid)\n",
    "writer.add_graph(model, example_data)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "875f6550-2c63-4e7e-848e-1d140faeacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7770357704162598 0.7237999987602234\n",
      "1 0.5766548186540603 0.7952999985218048\n",
      "2 0.5585945546627045 0.804200005531311\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 3;\n",
    "num_runs = 1\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = Mnist_CNN()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12084ac-bdd6-4f4a-890a-24112421d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 15;\n",
    "num_runs = 5\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = get_model()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263a028d-4950-4a1d-99da-e3cfde73a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Lambda: 1-1                            --\n",
       "├─Conv2d: 1-2                            160\n",
       "├─ReLU: 1-3                              --\n",
       "├─Conv2d: 1-4                            1,450\n",
       "├─ReLU: 1-5                              --\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─Flatten: 1-7                           --\n",
       "├─Linear: 1-8                            4,910\n",
       "├─LogSoftmax: 1-9                        --\n",
       "├─Lambda: 1-10                           --\n",
       "=================================================================\n",
       "Total params: 6,520\n",
       "Trainable params: 6,520\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(get_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a360e6-7bc5-432c-91e2-7668f4d33709",
   "metadata": {},
   "source": [
    "learning rate = 0.05 <br>\n",
    "mnist16 is with a 3x3 and 5x5 conv layers with stride 2 and then MaxPool(4) into an FC ~ 3.5k param <br>\n",
    "mnist17 is with a 3x3 and 5x5 conv layers with stride 2 and then MaxPool(3) into an FC ~ 5.5k param <br>\n",
    "mnist18 is with 3x3 and 5x5 conv layer with strides 1 and 2, followed by two 3x3 layers with strides 2 and 1, followed by a maxpool(3) into an FC ~ 6.6k param <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f20c74-e7e3-4307-8e4d-9b3c1ccdba25",
   "metadata": {},
   "source": [
    "# CNN with 3x3 and 5x5 within the same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8a6f22-a11e-4165-ade8-c0ed007e311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "writer = SummaryWriter(\"runs/mnist18\")\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def init_weights(mod):\n",
    "    if (type(mod) == nn.Conv2d) or (type(mod) == nn.Linear):\n",
    "        torch.nn.init.orthogonal_(mod.weight)\n",
    "\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2)\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight)\n",
    "        self.conv4 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "\n",
    "        self.lin1 = nn.Linear(10*4*4 + 10*4*4,10)\n",
    "        torch.nn.init.orthogonal_(self.lin1.weight)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb1 = F.relu(self.conv1(xb))\n",
    "        xb1 = F.relu(self.conv3(xb1))\n",
    "        xb2 = F.relu(self.conv2(xb))\n",
    "        xb2 = F.relu(self.conv4(xb2))\n",
    "        xb1 = F.max_pool2d(xb1, 3)\n",
    "        xb2 = F.max_pool2d(xb2, 3)\n",
    "        xb1 = xb1.view(xb.size(0), -1)\n",
    "        xb2 = xb2.view(xb.size(0), -1)\n",
    "        xb = torch.cat((xb1,xb2),1)\n",
    "        xb = self.lin1(xb)\n",
    "        xb = F.log_softmax(xb,dim=1)\n",
    "        return xb\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.05\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        Lambda(preprocess),\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "        #nn.ReLU(),\n",
    "        nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.AvgPool2d(4),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(10*7*7,10),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "        Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    )\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    n_total_steps = len(train_dl)\n",
    "    running_loss = 0.0\n",
    "    list_loss=[]\n",
    "    list_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            i+=1\n",
    "            running_loss += loss_batch(model,loss_func,xb, yb, opt)[0]\n",
    "            if i%100==0:\n",
    "                writer.add_scalar('training loss',running_loss/100, epoch*n_total_steps + i)\n",
    "                list_loss.append(running_loss/100)\n",
    "                running_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        writer.add_scalar('testing acc', val_acc, epoch)\n",
    "        list_acc.append(val_acc)\n",
    "        print(epoch, val_loss,val_acc)\n",
    "    return list_loss,list_acc\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dde690ff-0831-4a82-8096-ab95bb51ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c0f91c7-2f92-4cc9-a78b-603c69f22484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Mnist_CNN                                --\n",
       "├─Conv2d: 1-1                            160\n",
       "├─Conv2d: 1-2                            1,450\n",
       "├─Conv2d: 1-3                            416\n",
       "├─Conv2d: 1-4                            1,450\n",
       "├─Linear: 1-5                            3,210\n",
       "=================================================================\n",
       "Total params: 6,686\n",
       "Trainable params: 6,686\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8830-9778-4136-8f0a-ec5c8e120425",
   "metadata": {},
   "source": [
    "###### total_epochs = 15;\n",
    "num_runs = 5\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = Mnist_CNN()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a83218-12fa-4d8a-b2a1-cda65aef8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "examples = iter(train_dl)\n",
    "example_data, example_targets = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fa535d-f4d4-4ed2-95e5-336fe6eb45dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328f357-f455-411d-8727-4e33e8497add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
