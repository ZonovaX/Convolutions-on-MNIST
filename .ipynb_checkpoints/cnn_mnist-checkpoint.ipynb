{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb0372a-6544-41b7-8ced-99665ec6e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604b7449-1239-4565-be64-041b4bc8ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b8685-2e6f-4030-aa58-0ebbe74dd377",
   "metadata": {},
   "source": [
    "the 3 total conv layers are 1x16 -> 16x16 -> 16x10, and in the two conv case we use first and last <br>\n",
    "\n",
    "using lr = 0.001 <br>\n",
    "mnist6 (and mnist13) is with 2 conv layers with stride 1 and then AvgPool(4) into FC, ~6.5k param <br>\n",
    "mnist7 is with 2 conv layers with stride 2 and no pooling into FC ~6.5k param<br>\n",
    "mnist8 is with 3 conv layers with stride 2 and no pooling into FC ~5.5k param<br>\n",
    "mnist14 is with 2 conv layers with stride 1 and then MaxPool(4) into FC <br>\n",
    "using lr = 0.05 <br>\n",
    "mnist5 (and mnist10) is with all 3 conv layers at stride 2 and then 4 pooling, no FC, ~4k param <br>\n",
    "mnist9 is with 3 conv layers with stride 2 and no pooling into FC ~ 5.5k param <br>\n",
    "mnist11 is with 2 conv layers with stride 2 and no pooling into FC ~ 6.5k param <br>\n",
    "mnist12 is with 2 conv layers with stride 1 and then AvgPool(4) into an FC ~ 6.5k param <br>\n",
    "mnist15 is with 2 conv layers with stride 1 and then MaxPool(4) into an FC ~ 6.5k param <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc51d05-f765-4855-ba4d-1c9b3991beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "writer = SummaryWriter(\"runs/mnist15\")\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def init_weights(mod):\n",
    "    if (type(mod) == nn.Conv2d) or (type(mod) == nn.Linear):\n",
    "        torch.nn.init.orthogonal_(mod.weight)\n",
    "\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv3.weight)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return F.log_softmax(xb.view(-1, xb.size(1)), dim=1)\n",
    "\"\"\"\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.05\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\"\"\"\n",
    "\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        Lambda(preprocess),\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "        #nn.ReLU(),\n",
    "        nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.AvgPool2d(4),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(10*7*7,10),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "        Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    )\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    n_total_steps = len(train_dl)\n",
    "    running_loss = 0.0\n",
    "    list_loss=[]\n",
    "    list_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            i+=1\n",
    "            running_loss += loss_batch(model,loss_func,xb, yb, opt)[0]\n",
    "            if i%100==0:\n",
    "                writer.add_scalar('training loss',running_loss/100, epoch*n_total_steps + i)\n",
    "                list_loss.append(running_loss/100)\n",
    "                running_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        writer.add_scalar('testing acc', val_acc, epoch)\n",
    "        list_acc.append(val_acc)\n",
    "        print(epoch, val_loss,val_acc)\n",
    "    return list_loss,list_acc\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e35af2d-fb96-493e-978c-3039dd44fd94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dl, valid_dl \u001b[38;5;241m=\u001b[39m get_data(training_data,test_data,batch_size)\n\u001b[1;32m----> 2\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m example_data, example_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(examples)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Mnist_CNN()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "examples = iter(train_dl)\n",
    "example_data, example_targets = next(examples)\n",
    "model = Mnist_CNN()\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images_5',img_grid)\n",
    "writer.add_graph(model, example_data)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "875f6550-2c63-4e7e-848e-1d140faeacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7770357704162598 0.7237999987602234\n",
      "1 0.5766548186540603 0.7952999985218048\n",
      "2 0.5585945546627045 0.804200005531311\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 3;\n",
    "num_runs = 1\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = Mnist_CNN()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12084ac-bdd6-4f4a-890a-24112421d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 15;\n",
    "num_runs = 5\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = get_model()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263a028d-4950-4a1d-99da-e3cfde73a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Lambda: 1-1                            --\n",
       "├─Conv2d: 1-2                            160\n",
       "├─ReLU: 1-3                              --\n",
       "├─Conv2d: 1-4                            1,450\n",
       "├─ReLU: 1-5                              --\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─Flatten: 1-7                           --\n",
       "├─Linear: 1-8                            4,910\n",
       "├─LogSoftmax: 1-9                        --\n",
       "├─Lambda: 1-10                           --\n",
       "=================================================================\n",
       "Total params: 6,520\n",
       "Trainable params: 6,520\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(get_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a360e6-7bc5-432c-91e2-7668f4d33709",
   "metadata": {},
   "source": [
    "learning rate = 0.05 <br>\n",
    "mnist16 is with a 3x3 and 5x5 conv layers with stride 2 and then MaxPool(4) into an FC ~ 3.5k param <br>\n",
    "mnist17 is with a 3x3 and 5x5 conv layers with stride 2 and then MaxPool(3) into an FC ~ 5.5k param <br>\n",
    "mnist18 is with 3x3 and 5x5 conv layer with strides 1 and 2, followed by two 3x3 layers with strides 2 and 1, followed by a maxpool(3) into an FC ~ 6.6k param <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f20c74-e7e3-4307-8e4d-9b3c1ccdba25",
   "metadata": {},
   "source": [
    "# CNN with 3x3 and 5x5 within the same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8a6f22-a11e-4165-ade8-c0ed007e311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "writer = SummaryWriter(\"runs/mnist18\")\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def init_weights(mod):\n",
    "    if (type(mod) == nn.Conv2d) or (type(mod) == nn.Linear):\n",
    "        torch.nn.init.orthogonal_(mod.weight)\n",
    "\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2)\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight)\n",
    "        self.conv4 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "\n",
    "        self.lin1 = nn.Linear(10*4*4 + 10*4*4,10)\n",
    "        torch.nn.init.orthogonal_(self.lin1.weight)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb1 = F.relu(self.conv1(xb))\n",
    "        xb1 = F.relu(self.conv3(xb1))\n",
    "        xb2 = F.relu(self.conv2(xb))\n",
    "        xb2 = F.relu(self.conv4(xb2))\n",
    "        xb1 = F.max_pool2d(xb1, 3)\n",
    "        xb2 = F.max_pool2d(xb2, 3)\n",
    "        xb1 = xb1.view(xb.size(0), -1)\n",
    "        xb2 = xb2.view(xb.size(0), -1)\n",
    "        xb = torch.cat((xb1,xb2),1)\n",
    "        xb = self.lin1(xb)\n",
    "        xb = F.log_softmax(xb,dim=1)\n",
    "        return xb\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.05\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        Lambda(preprocess),\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "        #nn.ReLU(),\n",
    "        nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        #nn.AvgPool2d(4),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(10*7*7,10),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "        Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    )\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    n_total_steps = len(train_dl)\n",
    "    running_loss = 0.0\n",
    "    list_loss=[]\n",
    "    list_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            i+=1\n",
    "            running_loss += loss_batch(model,loss_func,xb, yb, opt)[0]\n",
    "            if i%100==0:\n",
    "                writer.add_scalar('training loss',running_loss/100, epoch*n_total_steps + i)\n",
    "                list_loss.append(running_loss/100)\n",
    "                running_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        writer.add_scalar('testing acc', val_acc, epoch)\n",
    "        list_acc.append(val_acc)\n",
    "        print(epoch, val_loss,val_acc)\n",
    "    return list_loss,list_acc\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dde690ff-0831-4a82-8096-ab95bb51ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c0f91c7-2f92-4cc9-a78b-603c69f22484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Mnist_CNN                                --\n",
       "├─Conv2d: 1-1                            160\n",
       "├─Conv2d: 1-2                            1,450\n",
       "├─Conv2d: 1-3                            416\n",
       "├─Conv2d: 1-4                            1,450\n",
       "├─Linear: 1-5                            3,210\n",
       "=================================================================\n",
       "Total params: 6,686\n",
       "Trainable params: 6,686\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8830-9778-4136-8f0a-ec5c8e120425",
   "metadata": {},
   "source": [
    "total_epochs = 15;\n",
    "num_runs = 5\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = Mnist_CNN()\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a83218-12fa-4d8a-b2a1-cda65aef8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "examples = iter(train_dl)\n",
    "example_data, example_targets = next(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac0c3d-f3fe-47d1-bf0e-7a7bd5ae78a4",
   "metadata": {},
   "source": [
    "# Imposing positive and negative ReLu's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e6f23-1749-4f75-9c19-5bf3f5786446",
   "metadata": {},
   "source": [
    "mnist19 is 2 3x3 conv layers with stride 1 and then MaxPool(4) into an FC and then ReLU into another FC ~ 50k param <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "149e8dec-4824-45ba-99d1-f1d773777f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "writer = SummaryWriter(\"runs/mnist21\")\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "class nReLU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            \n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        #return torch.where(data < 0.0, 0.0, -1*data)\n",
    "        return torch.where(data < 0.0, data, 0)\n",
    "    \n",
    "nReLU_func = nReLU() #instantiating the NReLU func to be used in model definition. \n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "def init_weights(mod):\n",
    "    if (type(mod) == nn.Conv2d) or (type(mod) == nn.Linear):\n",
    "        torch.nn.init.orthogonal_(mod.weight)\n",
    "\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1)\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight)\n",
    "        self.lin1 = nn.Linear(10*7*7,10*10)\n",
    "        torch.nn.init.orthogonal_(self.lin1.weight)\n",
    "        self.lin2 = nn.Linear(10*10,10)\n",
    "        torch.nn.init.orthogonal_(self.lin2.weight)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        b_size = xb.size(0)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.max_pool2d(xb, 4)\n",
    "        xb = xb.view(b_size, -1) #flatten\n",
    "        xb = self.lin1(xb)\n",
    "        xb1 = F.relu(xb[:,:80])\n",
    "        xb2 = nReLU_func(xb[:,80:])\n",
    "        xb[:,:80] = xb1\n",
    "        xb[:,80:] = xb2\n",
    "        xb = self.lin2(xb)\n",
    "        xb = F.log_softmax(xb, dim=1)\n",
    "        return xb\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.05\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_model():\n",
    "    model = nn.Sequential(\n",
    "        Lambda(preprocess),\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 10, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(10*7*7,10*10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10*10,10),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "        Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    )\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\"\"\"\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    n_total_steps = len(train_dl)\n",
    "    running_loss = 0.0\n",
    "    list_loss=[]\n",
    "    list_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            i+=1\n",
    "            running_loss += loss_batch(model,loss_func,xb, yb, opt)[0]\n",
    "            if i%100==0:\n",
    "                writer.add_scalar('training loss',running_loss/100, epoch*n_total_steps + i)\n",
    "                list_loss.append(running_loss/100)\n",
    "                running_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        writer.add_scalar('testing acc', val_acc, epoch)\n",
    "        list_acc.append(val_acc)\n",
    "        print(epoch, val_loss,val_acc)\n",
    "    return list_loss,list_acc\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5e29bae-29f1-481d-ad11-4f00d83364c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Mnist_CNN                                --\n",
       "├─Conv2d: 1-1                            160\n",
       "├─Conv2d: 1-2                            1,450\n",
       "├─Linear: 1-3                            49,100\n",
       "├─Linear: 1-4                            1,010\n",
       "=================================================================\n",
       "Total params: 51,720\n",
       "Trainable params: 51,720\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f7ffb99-56ef-4687-90ce-030498bdd8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3528710898756981 0.8780999982357025\n",
      "1 0.30033621162176133 0.8878999996185303\n",
      "2 0.30396718174219134 0.8926999962329865\n",
      "3 0.26918351650238037 0.9050999963283539\n",
      "4 0.2741980841755867 0.902799996137619\n",
      "5 0.28425645679235456 0.8978999960422516\n",
      "6 0.25823558822274206 0.9073999977111816\n",
      "7 0.2697607466578484 0.9037999999523163\n",
      "8 0.26874974489212033 0.9028999972343444\n",
      "9 0.26811496645212174 0.9079999935626983\n",
      "10 0.2711089074611664 0.9100000035762786\n",
      "11 0.26837310791015623 0.9086000001430512\n",
      "12 0.27104574859142305 0.9051999986171723\n",
      "13 0.2840476480126381 0.9074000024795532\n",
      "14 0.28137137115001676 0.9076999986171722\n",
      "0 0.3406068882346153 0.8820999979972839\n",
      "1 0.3314118778705597 0.8807999956607818\n",
      "2 0.2927394625544548 0.8957000017166138\n",
      "3 0.29585913211107256 0.8941000008583069\n",
      "4 0.2996776965260506 0.891599999666214\n",
      "5 0.31356619954109194 0.8905999994277954\n",
      "6 0.26817037135362626 0.9030999934673309\n",
      "7 0.27139764845371245 0.9057000029087067\n",
      "8 0.2740314742922783 0.9032999992370605\n",
      "9 0.2645185101032257 0.9039999997615814\n",
      "10 0.26891415178775785 0.900900000333786\n",
      "11 0.286752772629261 0.9039999985694885\n",
      "12 0.27391990035772323 0.9075999975204467\n",
      "13 0.2636294111609459 0.9102999973297119\n",
      "14 0.26332025706768036 0.9100999987125397\n",
      "0 0.3849983003735542 0.8544000041484833\n",
      "1 0.3274717673659325 0.8779999947547913\n",
      "2 0.31533320635557177 0.8837999999523163\n",
      "3 0.31400481551885606 0.8899999988079071\n",
      "4 0.28993107229471204 0.8940999984741211\n",
      "5 0.2940969654917717 0.8912999951839446\n",
      "6 0.29551507234573365 0.8914999961853027\n",
      "7 0.30268431156873704 0.8897999954223633\n",
      "8 0.28361828714609144 0.8964999985694885\n",
      "9 0.2848674374818802 0.8968999958038331\n",
      "10 0.3035892537236214 0.8921999955177307\n",
      "11 0.28633579194545744 0.8979999983310699\n",
      "12 0.28102098643779755 0.8988999974727631\n",
      "13 0.290989485681057 0.8939999997615814\n",
      "14 0.27005391508340837 0.9031000018119812\n",
      "0 0.3674909844994545 0.8696999990940094\n",
      "1 0.3069927203655243 0.890999995470047\n",
      "2 0.27707195073366164 0.8970999991893769\n",
      "3 0.27104668140411375 0.9023000025749206\n",
      "4 0.27091289579868316 0.90289999127388\n",
      "5 0.28228403478860853 0.8993999993801117\n",
      "6 0.2626178941130638 0.9069999980926514\n",
      "7 0.2715693062543869 0.9031000018119812\n",
      "8 0.25869672298431395 0.908499995470047\n",
      "9 0.27020229190587997 0.9033999979496002\n",
      "10 0.268466562628746 0.9072000002861023\n",
      "11 0.28041263312101367 0.9059999978542328\n",
      "12 0.27637360215187073 0.9084999978542327\n",
      "13 0.2656609123945236 0.9123999989032745\n",
      "14 0.3075835755467415 0.9053999960422516\n",
      "0 0.39274419903755187 0.8520000040531158\n",
      "1 0.33243511110544205 0.8758000004291534\n",
      "2 0.3138250055909157 0.8896999979019165\n",
      "3 0.29637931525707245 0.8955999982357025\n",
      "4 0.29627933114767074 0.8939999997615814\n",
      "5 0.28093703925609587 0.8955999994277954\n",
      "6 0.2956758540868759 0.8985999989509582\n",
      "7 0.27782925248146056 0.9000999999046325\n",
      "8 0.3352012971043587 0.872700001001358\n",
      "9 0.28155430912971496 0.9\n",
      "10 0.29210829466581345 0.8977999949455261\n",
      "11 0.2827709862589836 0.8989999949932098\n",
      "12 0.2763934037089348 0.9010999977588654\n",
      "13 0.2809363393485546 0.9030999970436097\n",
      "14 0.29407733142375947 0.8953999924659729\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 15;\n",
    "num_runs = 5\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "for run in range(num_runs):\n",
    "    train_dl, valid_dl = get_data(training_data,test_data,batch_size)\n",
    "    model = Mnist_CNN() #use get_model() for normal ReLU model\n",
    "    opt = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n",
    "    temp_loss, temp_acc = fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "    all_loss.append(np.array(temp_loss))\n",
    "    all_acc.append(np.array(temp_acc))\n",
    "all_acc = np.array(all_acc)\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "mean_acc = all_acc.mean(0)\n",
    "mean_loss = all_loss.mean(0)\n",
    "for j in range(len(mean_loss)):\n",
    "    writer.add_scalar('Average Loss',mean_loss[j], j)\n",
    "for j in range(len(mean_acc)):\n",
    "    writer.add_scalar('Average Accuracy',mean_acc[j], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7545cb6-131b-48d5-afd0-8dfbc4ce4b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0330,  0.0309, -0.0378,  ..., -0.1094,  0.0334, -0.0159],\n",
       "        [-0.0025,  0.0799, -0.0049,  ..., -0.0381,  0.0245,  0.1445],\n",
       "        [-0.0260, -0.0018, -0.1091,  ..., -0.1087,  0.0327, -0.0337],\n",
       "        ...,\n",
       "        [ 0.0394,  0.0682, -0.0446,  ..., -0.0712, -0.0580, -0.0339],\n",
       "        [-0.0606, -0.0604, -0.0339,  ..., -0.0179, -0.1048,  0.0247],\n",
       "        [ 0.0240, -0.2018, -0.1550,  ..., -0.0152, -0.0158,  0.0557]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[7].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c44b37d1-b4b3-4ae8-b4e2-0c075a451c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[7].weight.data = model[7].weight.clamp(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c16f7bd9-4253-4fed-9d88-53c7857e8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = example_data.view(100,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a380ca76-e8b8-451a-996d-aca5c15ff7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:,:100].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f775a40-8650-434b-8deb-23655621880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 684])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:,100:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614440e-c1be-4fde-aff2-24b9c619cb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
