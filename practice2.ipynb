{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d30317-2b94-4a17-ac31-05d0afbb53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e600044f-5046-44e0-beba-239597fc8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc782d66-6c84-4968-98a8-f177955d0ecc",
   "metadata": {},
   "source": [
    "# Applying Introduction of PyTorch Tutorial to Fashion.MNist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26fe95c-4be1-481b-a545-d3ac5e42feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3569.0107, grad_fn=<NllLossBackward0>)\n",
      "tensor(230.3740, grad_fn=<NllLossBackward0>) tensor(0.7594)\n"
     ]
    }
   ],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "weights = torch.randn(28*28,10) #each image has 28x28 pixels\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10,requires_grad = True)\n",
    "lr = 0.001\n",
    "\n",
    "#\"xb @ weights + bias\" is our linear map and log_softmax is the nonlinear part. \n",
    "def model(xb):\n",
    "    return F.log_softmax(xb @ weights + bias,dim=1)\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "#checking loss before training\n",
    "print(loss_func(model(test_data.data.reshape(-1,28*28).float()),test_data.targets))\n",
    "\n",
    "batch_size = 100;\n",
    "total_epochs = 2\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    #breaking up training data into batch_size chunks, convert into a vector instead of a matrix, \n",
    "    #and turn the data into floats instead of integers.\n",
    "    for i in range(training_data.data.shape[0]//batch_size + 1):\n",
    "        xb=training_data.data[i*batch_size:(i+1)*batch_size].reshape(-1,28*28).float()\n",
    "        pred = model(xb)\n",
    "        yb = training_data.targets[i*batch_size:(i+1)*batch_size]\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias -= bias.grad * lr\n",
    "            bias.grad.zero_()\n",
    "        \n",
    "#checking loss after training\n",
    "print(loss_func(model(test_data.data.reshape(-1,28*28).float()),test_data.targets), accuracy(model(test_data.data.reshape(-1,28*28).float()),test_data.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7257b05-e683-4a84-a737-25fe43c0b9b2",
   "metadata": {},
   "source": [
    "# Refactoring PyTorch Tutorial Code on Fashion.MNist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04837802-29c7-48b5-89ce-baa435777481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 100;\n",
    "\n",
    "#defining dataset and dataloader\n",
    "train_ds = TensorDataset(training_data.data.reshape(-1,28*28).float(), training_data.targets)\n",
    "valid_ds = TensorDataset(test_data.data.reshape(-1,28*28).float(), test_data.targets)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(28*28,10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return F.log_softmax(self.lin(xb),dim=1)\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "lr = 0.001\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "def accuracy(xb, yb):\n",
    "    max_xb = torch.argmax(xb,dim=1)\n",
    "    return (max_xb == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def acc_batch(model, xb, yb):\n",
    "    acc = accuracy(model(xb),yb)\n",
    "    return acc, len(xb)\n",
    "        \n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss_batch(model,loss_func,xb, yb, opt)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb,yb in valid_dl])\n",
    "            accs, nums = zip(*[acc_batch(model, xb, yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums))/np.sum(nums)\n",
    "        val_acc = np.sum(np.multiply(accs,nums))/np.sum(nums)\n",
    "        \n",
    "        print(epoch, val_loss,val_acc)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78a6ebed-c8f3-4b9f-981b-1754f9e2ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 104.30940696716308 0.7477999973297119\n",
      "1 80.01938446044922 0.7752000033855438\n",
      "2 111.76534820556641 0.7363999998569488\n",
      "3 113.08875564575196 0.7674999988079071\n",
      "4 121.05397827148437 0.7722999978065491\n",
      "5 86.86415077209473 0.7899000000953674\n",
      "6 136.05244064331055 0.7884000015258789\n",
      "7 82.35311103820801 0.7968000006675721\n",
      "8 65.5009937286377 0.8271000015735627\n",
      "9 113.14762939453125 0.7515000021457672\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10;\n",
    "train_dl, valid_dl = get_data(train_ds,valid_ds,batch_size)\n",
    "model, opt = get_model()\n",
    "fit(total_epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e064174-1516-44ad-b5e4-46c63092d41b",
   "metadata": {},
   "source": [
    "# Copying Code from Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da050970-5e87-4f8d-a4a6-cd5d0c5b3f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5193.6035)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss_func(model(xb),yb).data for xb, yb in valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fc6bc2-148d-4a5f-995f-1d4537045e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e5484c-8d7c-4fc2-a696-133075cd346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced7a3da-2e7c-4466-a3d0-951806c986a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2214, grad_fn=<NllLossBackward0>) tensor(0.0952)\n",
      "tensor(7.5153, grad_fn=<NllLossBackward0>) tensor(0.9013)\n"
     ]
    }
   ],
   "source": [
    "#Getting data\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "\n",
    "#defining loss function, weights, and bias tensors\n",
    "loss_func = F.nll_loss\n",
    "weights = torch.randn(28*28,10) #each image has 28x28 pixels\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10,requires_grad = True)\n",
    "lr = 0.5\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "#\"xb @ weights + bias\" is our linear map and log_softmax is the nonlinear part. \n",
    "def model(xb):\n",
    "    return F.log_softmax(xb @ weights + bias,dim=0)\n",
    "\n",
    "#checking loss before training\n",
    "print(loss_func(model(x_valid.reshape(-1,28*28).float()),y_valid), accuracy(model(x_valid.reshape(-1,28*28).float()), y_valid))\n",
    "\n",
    "batch_size = 64;\n",
    "total_epochs = 2\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    #breaking up training data into batch_size chunks, convert into a vector instead of a matrix, \n",
    "    #and turn the data into floats instead of integers.\n",
    "    for i in range(x_train.shape[0]//batch_size + 1):\n",
    "        xb=x_train.data[i*batch_size:(i+1)*batch_size].reshape(-1,28*28).float()\n",
    "        pred = model(xb)\n",
    "        yb = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias -= bias.grad * lr\n",
    "            bias.grad.zero_()\n",
    "        \n",
    "#checking loss after training\n",
    "print(loss_func(model(x_valid),y_valid), accuracy(model(x_valid), y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
